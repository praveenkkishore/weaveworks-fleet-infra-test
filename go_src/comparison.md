# Terraform Architecture Comparison

## Request Profile

| Parameter | Value |
|---|---|
| **Customers** | 10,000 |
| **Devices per customer** | 100 |
| **Total resources** | 1,000,000 |
| **Request rate** | 500/min (~8.3/sec) |
| **Operation type** | Create / Edit / Delete (per customer site) |

---

## Head-to-Head Comparison

| Factor | Your Go SDK (`terraform-exec`) | Tofu-Controller (Flux) | Atlantis | Terraform Operator (GalleyBytes) | Crossplane TF Provider | Direct API (no TF) |
|---|---|---|---|---|---|---|
| **Can handle 500 req/min?** | âœ… Yes (worker pool) | âŒ No | âŒ No | âš ï¸ Barely | âš ï¸ Barely | âœ… Yes |
| **Max throughput** | ~50-100 req/min per worker Ã— N workers | ~10-20 req/min | ~20-30 req/min | ~30-50 req/min | ~20-40 req/min | ~500+ req/min |
| **Workers needed for 500/min** | 10-15 goroutines | 25-50 pods | 15-25 pods | 10-17 pods | 12-25 pods | 5-10 goroutines |
| **RAM per operation** | 200MB (TF process) | 200MB + 100MB (controller) | 200MB + 50MB | 200MB + 80MB | 150MB | 5-10MB |
| **RAM for 500/min** | 2-3 GB | 7-15 GB | 5-7 GB | 3-5 GB | 3-6 GB | 50-100 MB |
| **Disk per operation** | 200MB (provider cache) | 200MB + PVC | 200MB + clone | 200MB + PVC | 150MB | 0 |
| **Latency per op** | 20-60s | 30-120s (reconcile loop) | 30-90s | 20-60s | 20-60s | 1-5s |
| **State backend** | PG âœ… (existing Aurora) | K8s Secret / S3 | Any TF backend | Any TF backend | K8s Secret | Your own DB |
| **State locking** | PG advisory locks âœ… | K8s lease | Built-in | Built-in | K8s lease | Your own DB locks |
| **Kafka/Dapr trigger** | âœ… You wire it | âŒ Git only | âŒ PR only | âŒ CRD only | âŒ CRD only | âœ… You wire it |
| **Drift detection** | âŒ Manual | âœ… Auto reconcile | âŒ Manual | âœ… Auto reconcile | âœ… Auto reconcile | âŒ Manual |
| **Multi-vendor (Cato+Versa+...)** | âœ… Any TF provider | âœ… Any TF provider | âœ… Any TF provider | âœ… Any TF provider | âœ… Any TF provider | âŒ Code per vendor |
| **New vendor onboarding** | Add `.tf` template | Add `.tf` in Git | Add `.tf` in repo | Add `.tf` in CRD | Add `.tf` in CRD | Write new API client |
| **Infra you already have** | âœ… Go + PG + Kafka | âŒ Need Flux cluster | âŒ Need Atlantis server | âŒ Need operator | âŒ Need Crossplane | âœ… Go + PG + Kafka |
| **New infra needed** | None | Flux + controller + PVCs | Atlantis server + webhooks | Operator + CRDs + PVCs | Crossplane + provider | None |
| **K8s dependency** | âŒ Optional | âœ… Required | âŒ Optional | âœ… Required | âœ… Required | âŒ Optional |
| **Complexity** | Medium | High | Medium | Medium-High | High | Low (but per-vendor) |
| **Observability** | Your Prometheus + DB | Controller metrics | Webhooks + logs | Controller metrics | Crossplane metrics | Your Prometheus + DB |
| **Retry/FSM** | âœ… You build it (like your adapter) | âœ… Built-in reconcile | âš ï¸ Manual re-run | âœ… Built-in reconcile | âœ… Built-in reconcile | âœ… You build it |
| **Cost at scale** | $0 (runs in your pods) | $$$ (25-50 extra pods) | $$ (15-25 pods) | $$ (10-17 pods) | $$ (12-25 pods) | $0 (runs in your pods) |

---

## Throughput Bottleneck Analysis at 500 req/min

| Bottleneck | Go SDK | Tofu-Controller | Atlantis | TF Operator | Crossplane TF |
|---|---|---|---|---|---|
| `terraform init` (provider download) | âš ï¸ Cache per worker dir | âš ï¸ Per pod init | âš ï¸ Per workspace | âš ï¸ Per pod init | âš ï¸ Per pod init |
| Provider API rate limit (Cato/Versa) | ğŸ”´ **Real bottleneck** | ğŸ”´ Same | ğŸ”´ Same | ğŸ”´ Same | ğŸ”´ Same |
| State lock contention | âš ï¸ PG lock per customer | âš ï¸ K8s lease | âš ï¸ Backend lock | âš ï¸ Backend lock | âš ï¸ K8s lease |
| Disk I/O | âš ï¸ Provider binaries | ğŸ”´ PVC provisioning | âš ï¸ Git clone | ğŸ”´ PVC provisioning | âš ï¸ In-memory |
| Memory pressure | âš ï¸ 200MB Ã— workers | ğŸ”´ 300MB Ã— pods | âš ï¸ 250MB Ã— pods | âš ï¸ 280MB Ã— pods | âš ï¸ 150MB Ã— pods |
| Scheduling overhead | âœ… None (goroutines) | ğŸ”´ Pod scheduling | âœ… Thread pool | ğŸ”´ Pod scheduling | ğŸ”´ Pod scheduling |

> **Key insight**: At 500 req/min, the **real bottleneck is always the vendor API rate limit** (Cato, Versa, etc.), not the Terraform execution layer. Every option hits the same wall.

---

## Verdict

| If you need... | Use |
|---|---|
| **500 req/min + Kafka-driven + multi-vendor** | **Your Go SDK** â€” only option that checks all boxes without new infra |
| **GitOps + drift detection + low volume** | Tofu-Controller |
| **PR-based workflow + audit trail** | Atlantis |
| **K8s-native + reconciliation** | TF Operator or Crossplane |
| **Maximum throughput (no TF overhead)** | Direct API (but lose multi-vendor portability) |

---

## Recommended Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Kafka / Dapr PubSub                      â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚                â”‚                   â”‚
       â–¼                â–¼                   â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Site Setup   â”‚ â”‚ Tunnel Setup â”‚ â”‚  Device Programming   â”‚
â”‚  (Terraform)  â”‚ â”‚ (Terraform)  â”‚ â”‚  (Direct API / Go)    â”‚
â”‚              â”‚ â”‚              â”‚ â”‚                      â”‚
â”‚ 1 per cust   â”‚ â”‚ 2-4 per site â”‚ â”‚ 100 per customer     â”‚
â”‚ = 10K total  â”‚ â”‚ = 40K total  â”‚ â”‚ = 1,000,000 total    â”‚
â”‚ âœ… fits TF    â”‚ â”‚ âœ… fits TF    â”‚ â”‚ âŒ too many for TF    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚                â”‚                   â”‚
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
                  â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
                  â”‚ PostgreSQL â”‚
                  â”‚ TF State + â”‚
                  â”‚ App State  â”‚
                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Layer Recommendations

| Layer | Volume | Tool |
|---|---|---|
| **Site provisioning** (10K sites) | ~500 req/min peak | Go SDK (`terraform-exec`) + PG state |
| **Tunnel/BGP setup** (40K tunnels) | ~500 req/min peak | Go SDK (`terraform-exec`) + PG state |
| **Device programming** (1M devices) | ~500 req/min | Direct vendor API via SDWAN adapter dispatcher |
| **Config updates** (ongoing) | ~500 req/min | Direct vendor API via SDWAN adapter dispatcher |

### Why Go SDK Wins

1. **No new infrastructure** â€” reuses existing Go + PostgreSQL + Kafka stack
2. **Kafka-native** â€” only Go SDK and Direct API support Kafka triggers; all controllers need Git/CRD
3. **Resource efficient** â€” goroutines (8KB each) vs pods (200MB+ each)
4. **Already built and tested** â€” executor.go with PG state backend proven (Site 183007)
5. **Multi-vendor** â€” add new `.tf` templates for Cato, Versa, or any future vendor
6. **Observable** â€” state queryable via SQL, metrics via existing Prometheus setup

---

## State Backend: PostgreSQL vs S3

| Factor | PostgreSQL | S3 |
|---|---|---|
| Already available | âœ… Existing Aurora | âŒ Need new S3 + DynamoDB |
| Locking | âœ… Built-in advisory locks | âš ï¸ Needs DynamoDB table |
| Per-customer isolation | âœ… Schema/workspace per customer | âœ… Key prefix per customer |
| Latency | âœ… Same VPC, <1ms | âš ï¸ 5-20ms API call |
| Ops overhead | âœ… Zero â€” reuse existing DB | âŒ S3 bucket + DynamoDB + IAM |
| Cost at 10K customers | âœ… ~0 (10K rows in existing DB) | âš ï¸ Cheap but not free |
| State inspection | âœ… SQL query | âš ï¸ Download from S3 |
| Backup/Recovery | âœ… Comes with RDS backups | âœ… S3 versioning |

**Recommendation: PostgreSQL** â€” zero new infrastructure, built-in locking, already proven.
